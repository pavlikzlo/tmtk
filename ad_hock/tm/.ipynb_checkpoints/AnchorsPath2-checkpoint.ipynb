{
 "metadata": {
  "name": "",
  "signature": "sha256:6a87d61950ab05a999beb2e2f0c7f45dd22ffdab04a4806b328c78574afde911"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import scipy.io\n",
      "import scipy\n",
      "import numpy as np\n",
      "import scipy.sparse\n",
      "\n",
      "from __future__ import division\n",
      "import time\n",
      "import math\n",
      "from numpy.random import RandomState\n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infile = open('docword.nips.txt')\n",
      "num_docs, num_words, nnz = int(infile.readline()), int(infile.readline()), int(infile.readline())\n",
      "K = 100\n",
      "\n",
      "M = scipy.sparse.lil_matrix((num_words, num_docs))\n",
      "\n",
      "for l in infile:\n",
      "    d, w, v = [int(x) for x in l.split()]\n",
      "    M[w-1, d-1] = v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_vocab = 'vocab.nips.txt'\n",
      "output_matrix, output_vocab = 'M.trunc', 'V.trunc'\n",
      "\n",
      "cutoff = 50\n",
      "\n",
      "# Read in the vocabulary and build a symbol table mapping words to indices\n",
      "table = dict()\n",
      "numwords = 0\n",
      "with open(full_vocab, 'r') as file:\n",
      "    for line in file:\n",
      "        table[line.rstrip()] = numwords\n",
      "        numwords += 1\n",
      "\n",
      "remove_word = [False]*numwords\n",
      "\n",
      "# Read in the stopwords\n",
      "with open('stopwords.txt', 'r') as file:\n",
      "    for line in file:\n",
      "        if line.rstrip() in table:\n",
      "            remove_word[table[line.rstrip()]] = True\n",
      "\n",
      "if M.shape[0] != numwords:\n",
      "    print 'Error: vocabulary file has different number of words', M.shape, numwords\n",
      "    sys.exit()\n",
      "    \n",
      "print 'Number of words is ', numwords\n",
      "print 'Number of documents is ', M.shape[1]\n",
      "\n",
      "M = M.tocsr()\n",
      "\n",
      "new_indptr = np.zeros(M.indptr.shape[0], dtype=np.int32)\n",
      "new_indices = np.zeros(M.indices.shape[0], dtype=np.int32)\n",
      "new_data = np.zeros(M.data.shape[0], dtype=np.float64)\n",
      "\n",
      "indptr_counter = 1\n",
      "data_counter = 0\n",
      "\n",
      "for i in xrange(M.indptr.size - 1):\n",
      "\n",
      "    # if this is not a stopword\n",
      "    if not remove_word[i]:\n",
      "\n",
      "        # start and end indices for row i\n",
      "        start = M.indptr[i]\n",
      "        end = M.indptr[i + 1]\n",
      "        \n",
      "        # if number of distinct documents that this word appears in is >= cutoff\n",
      "        if (end - start) >= cutoff:\n",
      "            new_indptr[indptr_counter] = new_indptr[indptr_counter-1] + end - start\n",
      "            new_data[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.data[start:end]\n",
      "            new_indices[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.indices[start:end]\n",
      "            indptr_counter += 1\n",
      "        else:\n",
      "            remove_word[i] = True\n",
      "\n",
      "new_indptr = new_indptr[0:indptr_counter]\n",
      "new_indices = new_indices[0:new_indptr[indptr_counter-1]]\n",
      "new_data = new_data[0:new_indptr[indptr_counter-1]]\n",
      "\n",
      "M = scipy.sparse.csr_matrix((new_data, new_indices, new_indptr))\n",
      "M = M.tocsc()\n",
      "\n",
      "print 'New number of words is ', M.shape[0]\n",
      "print 'New number of documents is ', M.shape[1]\n",
      "\n",
      "# Output the new vocabulary\n",
      "output = open(output_vocab, 'w')\n",
      "row = 0\n",
      "with open(full_vocab, 'r') as file:\n",
      "    for line in file:\n",
      "        if not remove_word[row]:\n",
      "            output.write(line)\n",
      "        row += 1\n",
      "output.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of words is  12419\n",
        "Number of documents is  1500\n",
        "New number of words is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2940\n",
        "New number of documents is  1500\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Params():\n",
      "    def __init__(self):\n",
      "        self.anchor_thresh = 100\n",
      "        self.eps = 1e-06\n",
      "        self.max_threads = 2\n",
      "        self.checkpoint_prefix = 'checkpoint'\n",
      "        self.log_prefix = 'log'\n",
      "        self.new_dim = 1000\n",
      "        self.top_words = 10\n",
      "        self.dictionary_file = output_vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = Params()\n",
      "print \"identifying candidate anchors\"\n",
      "candidate_anchors = []\n",
      "#only accept anchors that appear in a significant number of docs\n",
      "for i in xrange(M.shape[0]):\n",
      "    if len(np.nonzero(M[i, :])[1]) > params.anchor_thresh:\n",
      "        candidate_anchors.append(i)\n",
      "print len(candidate_anchors), \"candidates\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "identifying candidate anchors\n",
        "1735"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " candidates\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_Q_matrix(M):\n",
      "    vocabSize, numdocs = M.shape[0], M.shape[1]\n",
      "    M = np.array(M.todense())\n",
      "    diag_M = np.zeros(vocabSize)\n",
      "\n",
      "    for column in M.T:\n",
      "        denom = column.sum() * (column.sum()-1)\n",
      "        diag_M += column * 1.0 / denom\n",
      "        column  /= sqrt(denom)\n",
      "    \n",
      "    Q = (np.dot(M, M.T) - np.diag(diag_M)) / numdocs\n",
      "    \n",
      "    return Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = generate_Q_matrix(M)\n",
      "print \"Q sum is\", Q.sum()\n",
      "V = Q.shape[0]\n",
      "print \"done reading documents\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Q sum is 1.0\n",
        "done reading documents\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print params.dictionary_file\n",
      "vocab = open(params.dictionary_file)\n",
      "vocab = vocab.read()\n",
      "vocab = vocab.strip()\n",
      "vocab = vocab.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "V.trunc\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gramm_shmit_step(M, basis, j, anchor_words, candidates, dist): \n",
      "    max_dist_idx = candidates[np.argmax([dist(M[i]) for i in candidates])]\n",
      "    if j >= 0: basis[j] = M[max_dist_idx]/np.sqrt(dist(M[max_dist_idx]))\n",
      "    \n",
      "    return M[max_dist_idx], max_dist_idx\n",
      "\n",
      "def Projection_Find(M_orig, r, candidates, dist=lambda x: np.dot(x, x)):\n",
      "    dim, M = M_orig.shape[1], M_orig.copy()    \n",
      "    anchor_words, anchor_indices, basis = np.zeros((r, dim)), np.zeros(r, dtype=np.int), np.zeros((r-1, dim))\n",
      "    \n",
      "    for j in range(-1, r - 1):\n",
      "        if j >= 0:\n",
      "            for i in candidates: M[i] -= anchor_words[0] if j == 0 else np.dot(M[i], basis[j-1]) * basis[j-1]    \n",
      "        anchor_words[j+1], anchor_indices[j+1] = gramm_shmit_step(M, basis, j, anchor_words, candidates, dist)\n",
      "        \n",
      "    return (anchor_words, list(anchor_indices))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 405
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Random_Projection(M, new_dim):\n",
      "    old_dim = M.shape[0]\n",
      "    R = np.searchsorted(np.cumsum([1./6, 2./3, 1./6]), RandomState(100).random_sample(new_dim*old_dim)) - 1\n",
      "    R = np.reshape(math.sqrt(3)*R, (new_dim, old_dim))\n",
      "    return np.dot(R, M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 406
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findAnchors(Q, K, params, candidates):\n",
      "    # row normalize Q\n",
      "    row_sums = Q.sum(1)    \n",
      "    Q = (Q.transpose() / row_sums).transpose()\n",
      "\n",
      "    # Reduced dimension random projection method for recovering anchor words\n",
      "    Q_red = Random_Projection(Q.T, params.new_dim).T\n",
      "    anchors, anchor_indices = Projection_Find(Q_red, K, candidates)\n",
      "\n",
      "    Q = (Q.transpose() * row_sums).transpose()\n",
      "\n",
      "    return anchor_indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 407
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anchors = findAnchors(Q, K, params, candidate_anchors)\n",
      "print '70: %s' % vocab[anchors[70]]\n",
      "print \"anchors are:\"\n",
      "for i, a in enumerate(anchors):\n",
      "    print i, vocab[a]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70: codes\n",
        "anchors are:\n",
        "0 neuroscience\n",
        "1 part\n",
        "2 iii\n",
        "3 cognitive\n",
        "4 paul\n",
        "5 science\n",
        "6 architecture\n",
        "7 theory\n",
        "8 visual\n",
        "9 processing\n",
        "10 policy\n",
        "11 head\n",
        "12 speaker\n",
        "13 spike\n",
        "14 character\n",
        "15 motion\n",
        "16 speech\n",
        "17 face\n",
        "18 chip\n",
        "19 classifier\n",
        "20 control\n",
        "21 boolean\n",
        "22 teacher\n",
        "23 missing\n",
        "24 sound\n",
        "25 controller\n",
        "26 view\n",
        "27 kernel\n",
        "28 word\n",
        "29 orientation\n",
        "30 expert\n",
        "31 batch\n",
        "32 motor\n",
        "33 clustering\n",
        "34 winner\n",
        "35 hopfield\n",
        "36 channel\n",
        "37 convex\n",
        "38 member\n",
        "39 rules\n",
        "40 letter\n",
        "41 user\n",
        "42 carlo\n",
        "43 penalty\n",
        "44 robot\n",
        "45 contour\n",
        "46 operator\n",
        "47 synapses\n",
        "48 loss\n",
        "49 rbf\n",
        "50 tree\n",
        "51 rotation\n",
        "52 separation\n",
        "53 concept\n",
        "54 taylor\n",
        "55 light\n",
        "56 ensemble\n",
        "57 string\n",
        "58 mutual\n",
        "59 extra\n",
        "60 interpolation\n",
        "61 trajectory\n",
        "62 oscillation\n",
        "63 eye\n",
        "64 content\n",
        "65 building\n",
        "66 module\n",
        "67 matching\n",
        "68 inverse\n",
        "69 processor\n",
        "70 codes\n",
        "71 attention\n",
        "72 subspace\n",
        "73 capacity\n",
        "74 tractable\n",
        "75 weighting\n",
        "76 utility\n",
        "77 perceptton\n",
        "78 sensor\n",
        "79 predictor\n",
        "80 retina\n",
        "81 display\n",
        "82 weak\n",
        "83 regular\n",
        "84 evidence\n",
        "85 moment\n",
        "86 context\n",
        "87 adaptation\n",
        "88 entropy\n",
        "89 ann\n",
        "90 grid\n",
        "91 distances\n",
        "92 receptive\n",
        "93 recurrent\n",
        "94 polynomial\n",
        "95 organizing\n",
        "96 miller\n",
        "97 working\n",
        "98 similarity\n",
        "99 place\n"
       ]
      }
     ],
     "prompt_number": 410
    }
   ],
   "metadata": {}
  }
 ]
}