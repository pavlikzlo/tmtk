{
 "metadata": {
  "name": "",
  "signature": "sha256:f01081fe159fcb2db149f01a27184b23c7057db1e254b494b61f371e0e47db90"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import scipy.io\n",
      "import scipy\n",
      "import numpy as np\n",
      "import scipy.sparse\n",
      "\n",
      "from __future__ import division\n",
      "import time\n",
      "import math\n",
      "from numpy.random import RandomState\n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 412
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infile = open('docword.nips.txt')\n",
      "num_docs, num_words, nnz = int(infile.readline()), int(infile.readline()), int(infile.readline())\n",
      "K = 100\n",
      "\n",
      "M = scipy.sparse.lil_matrix((num_words, num_docs))\n",
      "\n",
      "for l in infile:\n",
      "    d, w, v = [int(x) for x in l.split()]\n",
      "    M[w-1, d-1] = v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 413
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_vocab = 'vocab.nips.txt'\n",
      "output_matrix, output_vocab = 'M.trunc', 'V.trunc'\n",
      "\n",
      "cutoff = 50\n",
      "\n",
      "# Read in the vocabulary and build a symbol table mapping words to indices\n",
      "table = dict()\n",
      "numwords = 0\n",
      "with open(full_vocab, 'r') as file:\n",
      "    for line in file:\n",
      "        table[line.rstrip()] = numwords\n",
      "        numwords += 1\n",
      "\n",
      "remove_word = [False]*numwords\n",
      "\n",
      "# Read in the stopwords\n",
      "with open('stopwords.txt', 'r') as file:\n",
      "    for line in file:\n",
      "        if line.rstrip() in table:\n",
      "            remove_word[table[line.rstrip()]] = True\n",
      "\n",
      "if M.shape[0] != numwords:\n",
      "    print 'Error: vocabulary file has different number of words', M.shape, numwords\n",
      "    sys.exit()\n",
      "    \n",
      "print 'Number of words is ', numwords\n",
      "print 'Number of documents is ', M.shape[1]\n",
      "\n",
      "M = M.tocsr()\n",
      "\n",
      "new_indptr = np.zeros(M.indptr.shape[0], dtype=np.int32)\n",
      "new_indices = np.zeros(M.indices.shape[0], dtype=np.int32)\n",
      "new_data = np.zeros(M.data.shape[0], dtype=np.float64)\n",
      "\n",
      "indptr_counter = 1\n",
      "data_counter = 0\n",
      "\n",
      "for i in xrange(M.indptr.size - 1):\n",
      "\n",
      "    # if this is not a stopword\n",
      "    if not remove_word[i]:\n",
      "\n",
      "        # start and end indices for row i\n",
      "        start = M.indptr[i]\n",
      "        end = M.indptr[i + 1]\n",
      "        \n",
      "        # if number of distinct documents that this word appears in is >= cutoff\n",
      "        if (end - start) >= cutoff:\n",
      "            new_indptr[indptr_counter] = new_indptr[indptr_counter-1] + end - start\n",
      "            new_data[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.data[start:end]\n",
      "            new_indices[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.indices[start:end]\n",
      "            indptr_counter += 1\n",
      "        else:\n",
      "            remove_word[i] = True\n",
      "\n",
      "new_indptr = new_indptr[0:indptr_counter]\n",
      "new_indices = new_indices[0:new_indptr[indptr_counter-1]]\n",
      "new_data = new_data[0:new_indptr[indptr_counter-1]]\n",
      "\n",
      "M = scipy.sparse.csr_matrix((new_data, new_indices, new_indptr))\n",
      "M = M.tocsc()\n",
      "\n",
      "print 'New number of words is ', M.shape[0]\n",
      "print 'New number of documents is ', M.shape[1]\n",
      "\n",
      "# Output the new vocabulary\n",
      "output = open(output_vocab, 'w')\n",
      "row = 0\n",
      "with open(full_vocab, 'r') as file:\n",
      "    for line in file:\n",
      "        if not remove_word[row]:\n",
      "            output.write(line)\n",
      "        row += 1\n",
      "output.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of words is  12419\n",
        "Number of documents is  1500\n",
        "New number of words is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2940\n",
        "New number of documents is  1500\n"
       ]
      }
     ],
     "prompt_number": 414
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Params():\n",
      "    def __init__(self):\n",
      "        self.anchor_thresh = 100\n",
      "        self.eps = 1e-06\n",
      "        self.max_threads = 2\n",
      "        self.checkpoint_prefix = 'checkpoint'\n",
      "        self.log_prefix = 'log'\n",
      "        self.new_dim = 1000\n",
      "        self.top_words = 10\n",
      "        self.dictionary_file = output_vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 415
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = Params()\n",
      "print \"identifying candidate anchors\"\n",
      "candidate_anchors = []\n",
      "#only accept anchors that appear in a significant number of docs\n",
      "for i in xrange(M.shape[0]):\n",
      "    if len(np.nonzero(M[i, :])[1]) > params.anchor_thresh:\n",
      "        candidate_anchors.append(i)\n",
      "print len(candidate_anchors), \"candidates\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "identifying candidate anchors\n",
        "1735"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " candidates\n"
       ]
      }
     ],
     "prompt_number": 416
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_Q_matrix(M):\n",
      "    vocabSize, numdocs = M.shape[0], M.shape[1]\n",
      "    M = np.array(M.todense())\n",
      "    diag_M = np.zeros(vocabSize)\n",
      "\n",
      "    for column in M.T:\n",
      "        denom = column.sum() * (column.sum()-1)\n",
      "        diag_M += column * 1.0 / denom\n",
      "        column  /= sqrt(denom)\n",
      "    \n",
      "    Q = (np.dot(M, M.T) - np.diag(diag_M)) / numdocs\n",
      "    \n",
      "    return Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 417
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = generate_Q_matrix(M)\n",
      "print \"Q sum is\", Q.sum()\n",
      "V = Q.shape[0]\n",
      "print \"done reading documents\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Q sum is 1.0\n",
        "done reading documents\n"
       ]
      }
     ],
     "prompt_number": 418
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print params.dictionary_file\n",
      "vocab = open(params.dictionary_file)\n",
      "vocab = vocab.read()\n",
      "vocab = vocab.strip()\n",
      "vocab = vocab.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "V.trunc\n"
       ]
      }
     ],
     "prompt_number": 419
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Random_Projection(M, new_dim):\n",
      "    old_dim = M.shape[0]\n",
      "    R = np.searchsorted(np.cumsum([1./6, 2./3, 1./6]), RandomState(100).random_sample(new_dim*old_dim)) - 1\n",
      "    R = np.reshape(math.sqrt(3)*R, (new_dim, old_dim))\n",
      "    return np.dot(R, M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 420
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gramm_shmit_step(M, basis, j, anchor_words, candidates, dist): \n",
      "    max_dist_idx = candidates[np.argmax([dist(M[i]) for i in candidates])]\n",
      "    if j >= 0: basis[j] = M[max_dist_idx]/np.sqrt(dist(M[max_dist_idx]))\n",
      "    \n",
      "    return M[max_dist_idx], max_dist_idx\n",
      "\n",
      "def Projection_Find(M_orig, r, candidates, dist=lambda x: np.dot(x, x)):\n",
      "    dim, M = M_orig.shape[1], M_orig.copy()    \n",
      "    anchor_words, anchor_indices, basis = np.zeros((r, dim)), np.zeros(r, dtype=np.int), np.zeros((r-1, dim))\n",
      "    \n",
      "    for j in range(-1, r - 1):\n",
      "        if j >= 0:\n",
      "            for i in candidates: M[i] -= anchor_words[0] if j == 0 else np.dot(M[i], basis[j-1]) * basis[j-1]    \n",
      "        anchor_words[j+1], anchor_indices[j+1] = gramm_shmit_step(M, basis, j, anchor_words, candidates, dist)\n",
      "        \n",
      "    return (anchor_words, list(anchor_indices))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 421
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findAnchors(Q, K, params, candidates):\n",
      "    # row normalize Q\n",
      "    row_sums = Q.sum(1)    \n",
      "    Q = (Q.transpose() / row_sums).transpose()\n",
      "\n",
      "    # Reduced dimension random projection method for recovering anchor words\n",
      "    Q_red = Random_Projection(Q.T, params.new_dim).T\n",
      "    anchors, anchor_indices = Projection_Find(Q_red, K, candidates)\n",
      "\n",
      "    Q = (Q.transpose() * row_sums).transpose()\n",
      "\n",
      "    return anchor_indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 422
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anchors = findAnchors(Q, K, params, candidate_anchors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 423
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab[anchors[70]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 425,
       "text": [
        "'codes'"
       ]
      }
     ],
     "prompt_number": 425
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}