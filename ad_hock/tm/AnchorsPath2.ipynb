{
 "metadata": {
  "name": "",
  "signature": "sha256:ad29322198b3b17571184a923cc0e05656093e95b913d87ba9d54fe0e504f476"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import scipy.io\n",
      "import scipy\n",
      "import numpy as np\n",
      "import scipy.sparse\n",
      "\n",
      "from __future__ import division\n",
      "import time\n",
      "import math\n",
      "from numpy.random import RandomState\n",
      "import itertools\n",
      "from multiprocessing import Pool\n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infile = open('docword.nips.txt')\n",
      "num_docs, num_words, nnz = int(infile.readline()), int(infile.readline()), int(infile.readline())\n",
      "K = 100\n",
      "\n",
      "M = scipy.sparse.lil_matrix((num_words, num_docs))\n",
      "\n",
      "for l in infile:\n",
      "    d, w, v = [int(x) for x in l.split()]\n",
      "    M[w-1, d-1] = v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_vocab = 'vocab.nips.txt'\n",
      "output_matrix, output_vocab = 'M.trunc', 'V.trunc'\n",
      "\n",
      "cutoff = 50\n",
      "\n",
      "# Read in the vocabulary and build a symbol table mapping words to indices\n",
      "table = dict()\n",
      "numwords = 0\n",
      "with open(full_vocab, 'r') as file:\n",
      "    for line in file:\n",
      "        table[line.rstrip()] = numwords\n",
      "        numwords += 1\n",
      "\n",
      "remove_word = [False]*numwords\n",
      "\n",
      "# Read in the stopwords\n",
      "with open('stopwords.txt', 'r') as file:\n",
      "    for line in file:\n",
      "        if line.rstrip() in table:\n",
      "            remove_word[table[line.rstrip()]] = True\n",
      "\n",
      "if M.shape[0] != numwords:\n",
      "    print 'Error: vocabulary file has different number of words', M.shape, numwords\n",
      "    sys.exit()\n",
      "    \n",
      "print 'Number of words is ', numwords\n",
      "print 'Number of documents is ', M.shape[1]\n",
      "\n",
      "M = M.tocsr()\n",
      "\n",
      "new_indptr = np.zeros(M.indptr.shape[0], dtype=np.int32)\n",
      "new_indices = np.zeros(M.indices.shape[0], dtype=np.int32)\n",
      "new_data = np.zeros(M.data.shape[0], dtype=np.float64)\n",
      "\n",
      "indptr_counter = 1\n",
      "data_counter = 0\n",
      "\n",
      "for i in xrange(M.indptr.size - 1):\n",
      "\n",
      "    # if this is not a stopword\n",
      "    if not remove_word[i]:\n",
      "\n",
      "        # start and end indices for row i\n",
      "        start = M.indptr[i]\n",
      "        end = M.indptr[i + 1]\n",
      "        \n",
      "        # if number of distinct documents that this word appears in is >= cutoff\n",
      "        if (end - start) >= cutoff:\n",
      "            new_indptr[indptr_counter] = new_indptr[indptr_counter-1] + end - start\n",
      "            new_data[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.data[start:end]\n",
      "            new_indices[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.indices[start:end]\n",
      "            indptr_counter += 1\n",
      "        else:\n",
      "            remove_word[i] = True\n",
      "\n",
      "new_indptr = new_indptr[0:indptr_counter]\n",
      "new_indices = new_indices[0:new_indptr[indptr_counter-1]]\n",
      "new_data = new_data[0:new_indptr[indptr_counter-1]]\n",
      "\n",
      "M = scipy.sparse.csr_matrix((new_data, new_indices, new_indptr))\n",
      "M = M.tocsc()\n",
      "\n",
      "print 'New number of words is ', M.shape[0]\n",
      "print 'New number of documents is ', M.shape[1]\n",
      "\n",
      "# Output the new vocabulary\n",
      "output = open(output_vocab, 'w')\n",
      "row = 0\n",
      "with open(full_vocab, 'r') as file:\n",
      "    for line in file:\n",
      "        if not remove_word[row]:\n",
      "            output.write(line)\n",
      "        row += 1\n",
      "output.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of words is  12419\n",
        "Number of documents is  1500\n",
        "New number of words is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2940\n",
        "New number of documents is  1500\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Params():\n",
      "    def __init__(self):\n",
      "        self.anchor_thresh = 100\n",
      "        self.eps = 1e-06\n",
      "        self.max_threads = 2\n",
      "        self.new_dim = 1000\n",
      "        self.top_words = 10\n",
      "        self.dictionary_file = output_vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = Params()\n",
      "print \"identifying candidate anchors\"\n",
      "candidate_anchors = []\n",
      "#only accept anchors that appear in a significant number of docs\n",
      "for i in xrange(M.shape[0]):\n",
      "    if len(np.nonzero(M[i, :])[1]) > params.anchor_thresh:\n",
      "        candidate_anchors.append(i)\n",
      "print len(candidate_anchors), \"candidates\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "identifying candidate anchors\n",
        "1735"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " candidates\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_Q_matrix(M):\n",
      "    vocabSize, numdocs = M.shape[0], M.shape[1]\n",
      "    M = np.array(M.todense())\n",
      "    diag_M = np.zeros(vocabSize)\n",
      "\n",
      "    for column in M.T:\n",
      "        denom = column.sum() * (column.sum()-1)\n",
      "        diag_M += column * 1.0 / denom\n",
      "        column  /= sqrt(denom)\n",
      "    \n",
      "    Q = (np.dot(M, M.T) - np.diag(diag_M)) / numdocs\n",
      "    \n",
      "    return Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = generate_Q_matrix(M)\n",
      "print \"Q sum is\", Q.sum()\n",
      "V = Q.shape[0]\n",
      "print \"done reading documents\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Q sum is 1.0\n",
        "done reading documents\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print params.dictionary_file\n",
      "vocab = open(params.dictionary_file)\n",
      "vocab = vocab.read()\n",
      "vocab = vocab.strip()\n",
      "vocab = vocab.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "V.trunc\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Random_Projection(M, new_dim=1000):\n",
      "    old_dim = M.shape[0]\n",
      "    R = np.searchsorted(np.cumsum([1./6, 2./3, 1./6]), RandomState(100).random_sample(new_dim*old_dim)) - 1\n",
      "    R = np.reshape(math.sqrt(3)*R, (new_dim, old_dim))\n",
      "    return np.dot(R, M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gramm_shmit_step(M, basis, j, anchor_words, candidates, dist): \n",
      "    max_dist_idx = candidates[np.argmax([dist(M[i]) for i in candidates])]\n",
      "    if j >= 0: basis[j] = M[max_dist_idx]/np.sqrt(dist(M[max_dist_idx]))\n",
      "    \n",
      "    return M[max_dist_idx], max_dist_idx\n",
      "\n",
      "def Projection_Find(M_orig, r, candidates, dist=lambda x: np.dot(x, x)):\n",
      "    dim, M = M_orig.shape[1], M_orig.copy()    \n",
      "    anchor_words, anchor_indices, basis = np.zeros((r, dim)), np.zeros(r, dtype=np.int), np.zeros((r-1, dim))\n",
      "    \n",
      "    for j in range(-1, r - 1):\n",
      "        if j >= 0:\n",
      "            for i in candidates: M[i] -= anchor_words[0] if j == 0 else np.dot(M[i], basis[j-1]) * basis[j-1]    \n",
      "        anchor_words[j+1], anchor_indices[j+1] = gramm_shmit_step(M, basis, j, anchor_words, candidates, dist)\n",
      "        \n",
      "    return (anchor_words, list(anchor_indices))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findAnchors(Q, K, params, candidates):\n",
      "    # row normalize Q\n",
      "    row_sums = Q.sum(1)    \n",
      "    Q = (Q.transpose() / row_sums).transpose()\n",
      "\n",
      "    # Reduced dimension random projection method for recovering anchor words\n",
      "    Q_red = Random_Projection(Q.T).T\n",
      "    anchors, anchor_indices = Projection_Find(Q_red, K, candidates)\n",
      "\n",
      "    Q = (Q.transpose() * row_sums).transpose()\n",
      "\n",
      "    return anchor_indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anchors = findAnchors(Q, K, params, candidate_anchors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab[anchors[70]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "'codes'"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This bad code -- RecoverL2 like RecoverL2scipy, but, worked faser then RecoverL2scipy.\n",
      "\n",
      "from scipy.optimize import fmin_slsqp\n",
      "\n",
      "def constrS(x):\n",
      "    return 1 - x.sum()\n",
      "\n",
      "def gen_const(dim):\n",
      "    g = lambda k: lambda x: x[k] \n",
      "    return [g(i) for i in range(dim)]\n",
      "\n",
      "constr, ieqcons = [constrS], gen_const(100)\n",
      "\n",
      "L2 = lambda qw, qanc: lambda x: ((np.dot(qanc.T, x)-qw)**2).sum()\n",
      "\n",
      "def RecoverL2scipy(Qw, Qanchors):     \n",
      "    c0 = np.random.random(100); c0 /= c0.sum();\n",
      "    return fmin_slsqp(L2(Qw, Qanchors), c0, eqcons=constr, ieqcons=ieqcons, iter=20, iprint=-1)\n",
      "\"\"\"\n",
      "\n",
      "def logsum_exp(y):\n",
      "    m = y.max()\n",
      "    return m + log((exp(y - m)).sum())\n",
      "\n",
      "\n",
      "def RecoverL2(y, x, eps=10**(-7)): \n",
      "    c1, c2 = 10**(-4), 0.75\n",
      "    \n",
      "    XX, XY, YY = dot(x, x.transpose()), dot(x, y), float(dot(y, y))\n",
      "\n",
      "    alpha = ones(x.shape[0])/x.shape[0]\n",
      "\n",
      "    log_alpha = log(alpha)\n",
      "\n",
      "    aXX, aXY = dot(alpha, XX), float(dot(alpha, XY))\n",
      "    aXXa = float(dot(aXX, alpha.transpose()))\n",
      "\n",
      "    grad, new_obj = 2*(aXX-XY), aXXa - 2*aXY + YY\n",
      "\n",
      "    stepsize, decreased = 1, False\n",
      "\n",
      "    while 1:\n",
      "        eta, old_alpha, old_log_alpha = stepsize, copy(alpha), copy(log_alpha)\n",
      "        if new_obj == 0 or stepsize == 0: break\n",
      "\n",
      "        log_alpha -= eta*grad\n",
      "        log_alpha -= logsum_exp(log_alpha)\n",
      "\n",
      "        alpha = exp(log_alpha)\n",
      "\n",
      "        aXX, aXY = dot(alpha, XX), float(dot(alpha, XY))\n",
      "        aXXa = float(dot(aXX, alpha.transpose()))\n",
      "\n",
      "        old_obj = new_obj\n",
      "        new_obj = aXXa - 2*aXY + YY\n",
      "        if not new_obj <= old_obj + c1*stepsize*dot(grad, alpha - old_alpha): \n",
      "            stepsize /= 2.0\n",
      "            alpha = old_alpha \n",
      "            log_alpha = old_log_alpha\n",
      "            new_obj = old_obj\n",
      "            decreased = True\n",
      "            continue\n",
      "\n",
      "        old_grad = copy(grad)\n",
      "        grad = 2 * (aXX - XY)\n",
      "        \n",
      "        if (not dot(grad, alpha - old_alpha) >= c2*dot(old_grad, alpha-old_alpha)) and (not decreased):\n",
      "            stepsize *= 2.0\n",
      "            alpha = old_alpha\n",
      "            log_alpha = old_log_alpha\n",
      "            grad = old_grad\n",
      "            new_obj = old_obj\n",
      "            continue\n",
      "\n",
      "        decreased = False\n",
      "\n",
      "        lam = copy(grad)\n",
      "        lam -= lam.min()\n",
      "\n",
      "        if (dot(alpha, lam) < eps):\n",
      "            break\n",
      "    \n",
      "    return alpha"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def RecoverA(Q, anchors, n_jobs=4):\n",
      "    V, K = Q.shape[0], len(anchors) \n",
      "    P_w = matrix(diag(dot(Q, ones(V))))\n",
      "    Q = (Q.transpose() / Q.T.sum(1)).transpose()\n",
      "        \n",
      "    A = P_w * matrix([RecoverL2(Q[w], Q[anchors]) for w in xrange(V)])\n",
      "     \n",
      "    return array(A / A.sum(0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = generate_Q_matrix(M)\n",
      "A = RecoverA(Q, anchors)\n",
      "\n",
      "topwords = np.argsort(A.T[70])[-params.top_words:][::-1]\n",
      "print vocab[anchors[70]], ':', ' '.join([vocab[w] for w in topwords])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "codes : code vector unit bit codes error information probability decoding coding\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#codes : code vector unit codes bit error information probability decoding coding"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}